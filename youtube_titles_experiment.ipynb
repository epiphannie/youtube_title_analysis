{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conda install altair --channel conda-forge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import csv\n",
    "import sys\n",
    "import nltk\n",
    "import pprint\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "input_file = 'USvideos.csv'\n",
    "pos_file = 'parts_of_speech.csv'\n",
    "\n",
    "with open(input_file, 'r') as youtube_data:\n",
    "    masterdata_csv = list(csv.reader(youtube_data))\n",
    "\n",
    "with open(pos_file, 'r') as parts_of_speech:\n",
    "    pos_load = csv.reader(parts_of_speech)\n",
    "    pos_dict = {}\n",
    "    for row in pos_load:\n",
    "        pos_dict[row[0]] = row[1]\n",
    "\n",
    "def run_query(query):\n",
    "    return pd.read_sql_query(query,db)\n",
    "\n",
    "def open_db(database):\n",
    "    db = sqlite3.connect(database)\n",
    "    db.execute('PRAGMA foreign_keys = ON;')\n",
    "    return db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Making an effort to decorate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sql_decorator(func):\n",
    "    \n",
    "    def sql_action(statement):\n",
    "        db = None\n",
    "        try:\n",
    "            db = open_db('youtube.db')\n",
    "            c = db.cursor()\n",
    "            func(c, statement)\n",
    "            db.commit()\n",
    "\n",
    "        except OSError as err:\n",
    "            print(\"OS error: {}\".format(err))\n",
    "\n",
    "        finally:\n",
    "            db.rollback()\n",
    "            db.close()\n",
    "\n",
    "    return sql_action\n",
    "\n",
    "@sql_decorator\n",
    "def run_sql(c, statement):\n",
    "    c.execute(statement)\n",
    "\n",
    "@sql_decorator\n",
    "def run_sql_many(c, statement):\n",
    "    c.executemany(statement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# youtube_pd = pd.read_csv('USvideos.csv', index_col = None, na_values = ['NA'])\n",
    "# masterdata_pd.head(n = 5)\n",
    "# masterdata_pd.tail(n =5)\n",
    "# filtered_yt = youtube_pd[(youtube_pd.title == 'something') & (youtube_pd.views > something else)]\n",
    "#filtered_yt = youtube_pd.filter(items = ['title', 'views'])\n",
    "# youtube_pd.describe() will summarize your data for you\n",
    "# youtube_pd['likes'].describe()\n",
    "# pandas has a merge function, like SQL joins. pd.merge(), supports left right inner outer\n",
    "# .pivot allows for the creation or pivot tables by indicating a row x colum array (child poverty shown with city as the rows and year as the columns)\n",
    "\n",
    "# Can I write derrived data back into a panda data set?\n",
    "# Where does this fit in with the requirements to use SQL?\n",
    "# How does this work library to library? Can I feed panda data into matplotlib? By subsetting, or indexing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pandas - Panel Data\n",
    "\n",
    "Rows are a ranges, columns are variables\n",
    "pd.read_csv()\n",
    "put the csv in the repo -- they want to see you download the data to the current working directory\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create database and master table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "try : \n",
    "    os.remove('youtube.db')\n",
    "except FileNotFoundError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_statement_master = '''\n",
    "    CREATE TABLE tblMasterData (\n",
    "    id INTEGER AUTOIMCREMENT PRIMARY KEY,\n",
    "    video_id TEXT,\n",
    "    trending_date TEXT,\n",
    "    title TEXT,\n",
    "    channel_title TEXT,\n",
    "    category_id INTEGER,\n",
    "    publish_time INTEGER,\n",
    "    tags TEXT,\n",
    "    views INTEGER,\n",
    "    likes INTEGER,\n",
    "    dislikes INTEGER,\n",
    "    comment_count INTEGER,\n",
    "    thumbnail_link TEXT,\n",
    "    comments_disabled TEXT,\n",
    "    ratings_disabled TEXT,\n",
    "    video_error_or_removed TEXT,\n",
    "    description TEXT)\n",
    "    '''\n",
    "\n",
    "run_sql(create_statement_master)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save column headers into a dictionary and delete column header row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = masterdata_csv[0]\n",
    "headers_dict = {}\n",
    "\n",
    "for count, value in enumerate(headers, 1):\n",
    "    headers_dict[value] = count -1\n",
    "# to keep from 0 indexing my columns\n",
    "\n",
    "del masterdata_csv[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load data into master table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "OperationalError",
     "evalue": "near \"(\": syntax error",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOperationalError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-b663a22f0d75>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m \"\"\"\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m \u001b[0mrun_sql_many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mload_data_master\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0mmaster_limit_5\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_sql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'SELECT * FROM tblMasterData LIMIT 5;'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-333450bf5111>\u001b[0m in \u001b[0;36msql_action\u001b[0;34m(statement, x)\u001b[0m\n\u001b[1;32m      7\u001b[0m             \u001b[0mc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcursor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m                 \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatement\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m                 \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatement\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-333450bf5111>\u001b[0m in \u001b[0;36mrun_sql_many\u001b[0;34m(c, statement, x)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m@\u001b[0m\u001b[0msql_decorator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mrun_sql_many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatement\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m     \u001b[0mc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecutemany\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstatement\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mOperationalError\u001b[0m: near \"(\": syntax error"
     ]
    }
   ],
   "source": [
    "\n",
    "load_data_master = \"\"\"\n",
    "('''\n",
    "INSERT INTO tblMasterData\n",
    "(video_id,\n",
    "trending_date,\n",
    "title,\n",
    "channel_title,\n",
    "category_id,\n",
    "publish_time,\n",
    "tags,\n",
    "views,\n",
    "likes,\n",
    "dislikes,\n",
    "comment_count,\n",
    "thumbnail_link,\n",
    "comments_disabled,\n",
    "ratings_disabled,\n",
    "video_error_or_removed,\n",
    "description)\n",
    "VALUES (?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?)\n",
    "''', \n",
    "masterdata_csv)\n",
    "\"\"\"\n",
    "\n",
    "run_sql_many(load_data_master)\n",
    "\n",
    "master_limit_5 = run_sql('SELECT * FROM tblMasterData LIMIT 5;')\n",
    "\n",
    "master_limit5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### After reviewing master data, create two tables based on function -- one static, one transactional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = open_db('youtube.db')\n",
    "c = db.cursor()\n",
    "\n",
    "create_statement_video = '''\n",
    "    CREATE TABLE tblVideos (\n",
    "    video_id TEXT PRIMARY KEY NOT NULL,\n",
    "    title TEXT NOT NULL,\n",
    "    channel_title TEXT NOT NULL,\n",
    "    publish_time INTEGER NOT NULL,\n",
    "    tags TEXT NOT NULL,\n",
    "    thumbnail_link TEXT NOT NULL,\n",
    "    comments_disabled TEXT NOT NULL,\n",
    "    ratings_disabled TEXT NOT NULL,\n",
    "    video_error_or_removed TEXT NOT NULL,\n",
    "    description TEXT NOT NULL)\n",
    "    '''\n",
    "\n",
    "c.execute(create_statement_video)\n",
    "\n",
    "create_statement_time = '''\n",
    "    CREATE TABLE tblTime (\n",
    "    id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "    video_id TEXT NOT NULL,\n",
    "    trending_date TEXT NOT NULL,\n",
    "    views INTEGER NOT NULL,\n",
    "    likes INTEGER NOT NULL,\n",
    "    dislikes INTEGER NOT NULL,\n",
    "    comment_count INTEGER NOT NULL,\n",
    "        FOREIGN KEY(video_id) REFERENCES tblVideos(video_id))\n",
    "    '''\n",
    "\n",
    "c.execute(create_statement_time)\n",
    "\n",
    "db.commit()\n",
    "\n",
    "db.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Date is stored in hard-to-read format. Transform date column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for row in masterdata_csv:\n",
    "    date = row[headers_dict['trending_date']]\n",
    "    updated_date = \"20\" + date[:2]\n",
    "    updated_date += \"-\"\n",
    "    updated_date += date[6:]\n",
    "    updated_date += \"-\"\n",
    "    updated_date += date[3:5]\n",
    "    row[headers_dict['trending_date']] = updated_date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Titles contain characters + and &. These will not be parseable. Replacing with 'and'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for row in masterdata_csv:\n",
    "    title = row[headers_dict['title']]\n",
    "    updated_title = re.sub('&|\\+', 'and', title)\n",
    "    row[headers_dict['title']] = updated_title"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Separate CSV file into lists to be loaded to each table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_data = []\n",
    "time_data = []\n",
    "\n",
    "for row in masterdata_csv:\n",
    "    video_entry = [row[headers_dict['video_id']], \n",
    "                   row[headers_dict['title']], \n",
    "                   row[headers_dict['channel_title']], \n",
    "                   row[headers_dict['publish_time']],\n",
    "                   row[headers_dict['tags']],\n",
    "                   row[headers_dict['thumbnail_link']],\n",
    "                   row[headers_dict['comments_disabled']],\n",
    "                   row[headers_dict['ratings_disabled']],\n",
    "                   row[headers_dict['video_error_or_removed']],\n",
    "                   row[headers_dict['description']]\n",
    "                  ]\n",
    "    \n",
    "    video_data.append(video_entry)\n",
    "    \n",
    "    time_entry = [row[headers_dict['video_id']], \n",
    "                  row[headers_dict['trending_date']], \n",
    "                  row[headers_dict['views']], \n",
    "                  row[headers_dict['likes']],\n",
    "                  row[headers_dict['dislikes']],\n",
    "                  row[headers_dict['comment_count']]\n",
    "                 ]\n",
    "    \n",
    "    time_data.append(time_entry)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load data to tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = open_db('youtube.db')\n",
    "c = db.cursor()\n",
    "\n",
    "c.executemany('''\n",
    "        INSERT OR REPLACE INTO tblVideos\n",
    "        (video_id,\n",
    "        title,\n",
    "        channel_title,\n",
    "        publish_time,\n",
    "        tags,\n",
    "        thumbnail_link,\n",
    "        comments_disabled,\n",
    "        ratings_disabled,\n",
    "        video_error_or_removed,\n",
    "        description)\n",
    "        VALUES (?,?,?,?,?,?,?,?,?,?)\n",
    "        ''', \n",
    "        video_data)\n",
    "\n",
    "db.commit()\n",
    "    \n",
    "c.executemany('''\n",
    "        INSERT INTO tblTime\n",
    "        (video_id,\n",
    "        trending_date,\n",
    "        views,\n",
    "        likes,\n",
    "        dislikes,\n",
    "        comment_count)\n",
    "        VALUES (?,?,?,?,?,?)\n",
    "        ''', \n",
    "        time_data)\n",
    "\n",
    "db.commit()\n",
    "\n",
    "videos_limit5 = run_query('SELECT * FROM tblVideos LIMIT 5;')\n",
    "\n",
    "time_limit5 = run_query ('SELECT * FROM tblTime LIMIT 5;')\n",
    "\n",
    "db.close()\n",
    "\n",
    "# to view a sample, uncomment below\n",
    "# videos_limit5\n",
    "# time_limit5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test for foreign key failure  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = open_db('youtube.db')\n",
    "c = db.cursor()\n",
    "\n",
    "try:\n",
    "    c.execute(\"INSERT INTO tblTime VALUES (?, ?, ?, ?, ?, ?, ?)\", (None, \"testy\", \"18.11.11\", 1, 1, 1, 1))\n",
    "except:\n",
    "    print(sys.exc_info())\n",
    "\n",
    "db.commit()\n",
    "db.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### ^ look at that! It failed!!! :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Titles appear to come in multiple segments, divided by special characters. Create Segments table to store segments of each title for analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = open_db('youtube.db')\n",
    "c = db.cursor()\n",
    "\n",
    "create_statement_segments = '''\n",
    "    CREATE TABLE tblSegments (\n",
    "    segment_id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "    video_id TEXT NOT NULL,\n",
    "    segment_text TEXT NOT NULL,\n",
    "    segment_structure TEXT,\n",
    "        FOREIGN KEY(video_id) REFERENCES tblVideos(video_id))\n",
    "    '''\n",
    "\n",
    "c.execute(create_statement_segments)\n",
    "\n",
    "db.commit()\n",
    "db.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define classes to facilitate analaysis...and for practice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Video:\n",
    "    \n",
    "    def __init__(self, video_id, title):\n",
    "        self.video_id = video_id\n",
    "        self.title = title\n",
    "    \n",
    "    def longest_word(self):\n",
    "        longest_length = 0\n",
    "        longest_word = ''\n",
    "        for word in self.title:\n",
    "            if len(word) > longest_length:\n",
    "                longest_length = len(word)\n",
    "                longest_word = word\n",
    "            else:\n",
    "                continue\n",
    "        return longest_word\n",
    "\n",
    "    def title_segments(self):\n",
    "        list = re.compile(\"(?:\\||(?:\\s-\\s)|â€”|:|\\(|\\)|\\[|\\]|{|})+\").split(self.title)\n",
    "        # ?: indicates a non-capture group so delimiters aren't saved. Now I know.\n",
    "        list = filter(lambda x: x != None, list)\n",
    "        #  python is returning None where the delimiter was. Removing Nones.\n",
    "        segments = []\n",
    "        for text in list:\n",
    "            text = text.strip()\n",
    "            if text == \"\":\n",
    "                continue\n",
    "            segments.append(Segment(self.video_id, text))\n",
    "        return segments\n",
    "\n",
    "    \n",
    "class Segment:\n",
    "    \n",
    "    def __init__(self, video_id, text):\n",
    "        self.video_id = video_id\n",
    "        self._text = text\n",
    "        \n",
    "    def text(self):\n",
    "        lower_words = self._text.lower()\n",
    "        return re.sub('[^A-Za-z0-9\\s\\-\\']+', '', lower_words)\n",
    "    \n",
    "    def words(self):\n",
    "        list = self.text().split()\n",
    "        list = filter(lambda x: x != None, list)\n",
    "        #  python is returning None where the delimiter was. Removing Nones.\n",
    "        segments = []\n",
    "        for item in list:\n",
    "            text = item.strip()\n",
    "            if text == \"\":\n",
    "                continue\n",
    "            segments.append(text)\n",
    "        return segments\n",
    "    \n",
    "    def parts_of_speech(self):\n",
    "        text = nltk.word_tokenize(self.text())\n",
    "        tagged_text = nltk.pos_tag(text)\n",
    "        #  creates list of tuples with (word, part of speech)\n",
    "        return [item[1] for item in tagged_text]\n",
    "\n",
    "class Title_glob:\n",
    "    \n",
    "    def __init__(self, glob):\n",
    "        self.glob = glob\n",
    "    \n",
    "    def word_list(self):\n",
    "        return self.glob\n",
    "    \n",
    "    def frequency_distribution(self):\n",
    "        return nltk.FreqDist(self.glob)\n",
    "    \n",
    "    def lexical_diversity(self):\n",
    "        return len(set(self.glob)) / len(self.glob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Select data from Videos table to parse into segments, generate parts of speech for each, and commit to Segments table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = open_db('youtube.db')\n",
    "c = db.cursor()\n",
    "\n",
    "c.execute(\"SELECT video_id, title FROM tblVideos;\")\n",
    "titles_list = c.fetchall()\n",
    "\n",
    "db.commit()\n",
    "db.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "segment_entries=[]\n",
    "\n",
    "for row in titles_list:\n",
    "    video = Video(row[0], row[1])\n",
    "    segments = video.title_segments()\n",
    "    for segment in segments:\n",
    "        segment_entry = []\n",
    "        segment_entry.append(segment.video_id)\n",
    "        segment_entry.append(segment.text())\n",
    "        segment_entry.append(\", \".join(segment.parts_of_speech()))\n",
    "        segment_entries.append(segment_entry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = open_db('youtube.db')\n",
    "c = db.cursor()\n",
    "\n",
    "c.executemany('''\n",
    "        INSERT INTO tblSegments\n",
    "        (video_id,\n",
    "        segment_text,\n",
    "        segment_structure)\n",
    "        VALUES (?,?, ?)\n",
    "        ''', \n",
    "        segment_entries)\n",
    "\n",
    "select_segments = run_query('SELECT * FROM tblSegments WHERE segment_id>=(abs(random()) % (SELECT max(segment_id)FROM tblSegments)) LIMIT 5')\n",
    "\n",
    "db.commit()\n",
    "db.close()\n",
    "\n",
    "select_segments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It doesn't know that all nouns aren't proper...bit of a bummer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = open_db('youtube.db')\n",
    "c = db.cursor()\n",
    "\n",
    "c.execute(\"SELECT video_id, segment_text, segment_id FROM tblSegments;\")\n",
    "segments_list = c.fetchall()\n",
    "\n",
    "db.commit()\n",
    "db.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Concatenate all segments for analysis as a whole"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lower_case(list):\n",
    "    return [w.lower() for w in list]\n",
    "\n",
    "all_segment_words = []\n",
    "\n",
    "for item in segments_list:\n",
    "    segment = Segment(item[0], item[1])\n",
    "    segment_words = segment.words()\n",
    "    for word in segment_words: \n",
    "        all_segment_words.append(word)\n",
    "\n",
    "all_segment_words = lower_case(all_segment_words)\n",
    "\n",
    "all_words = Title_glob(sorted(all_segment_words))\n",
    "# print(all_words.word_list())\n",
    "\n",
    "all_tokens = Title_glob(sorted(set(all_segment_words)))\n",
    "# print(all_tokens.word_list())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The X most common words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_dist = all_words.frequency_distribution()\n",
    "\n",
    "print(freq_dist.most_common(30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\"\"\n",
    "        The lexical diversity of the corpus is {}. \n",
    "        This is generated by dividing the length of the set of unique words over the length of  the set of all words.\"\"\"\n",
    "        .format(all_words.lexical_diversity()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "longest_word = ''\n",
    "longest_length = 0\n",
    "\n",
    "for word in all_tokens.word_list():\n",
    "    if len(word) > longest_length:\n",
    "        longest_word = word\n",
    "        longest_length = len(word)\n",
    "\n",
    "print(\"The longest word in the corpus is {} with a length of {} characters\".format(longest_word, longest_length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = open_db('youtube.db')\n",
    "c = db.cursor()\n",
    "\n",
    "c.execute(\"\"\"\n",
    "            SELECT count(segment_id) as number_of_segments, video_id \n",
    "            FROM tblSegments \n",
    "            GROUP BY video_id \n",
    "            ORDER BY number_of_segments desc\n",
    "            LIMIT 6;\n",
    "            \"\"\")\n",
    "\n",
    "segments_count = c.fetchall()\n",
    "\n",
    "db.commit()\n",
    "db.close()\n",
    "\n",
    "segments_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    db = open_db('youtube.db')\n",
    "    c = db.cursor()\n",
    "    sql = \"\"\"\n",
    "            SELECT count(segment_id) as number_of_segments, video_id \n",
    "            FROM tblSegments \n",
    "            GROUP BY video_id;\n",
    "            \"\"\"\n",
    "\n",
    "    df = pd.read_sql(sql, db)\n",
    "\n",
    "finally:\n",
    "    db.close()\n",
    "\n",
    "print(df.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The mean number of segments per title is 1.855141"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    db = open_db('youtube.db')\n",
    "    c = db.cursor()\n",
    "    sql = \"\"\"\n",
    "            SELECT count(video_id) as number_of_pattern_occurances, segment_structure\n",
    "            FROM tblSegments \n",
    "            GROUP BY segment_structure\n",
    "            ORDER BY number_of_pattern_occurances DESC\n",
    "            LIMIT 10;\n",
    "            \"\"\"\n",
    "\n",
    "    df = pd.read_sql(sql, db)\n",
    "\n",
    "finally:\n",
    "    db.close()\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    db = open_db('youtube.db')\n",
    "    c = db.cursor()\n",
    "    sql_repeated = \"\"\"\n",
    "            SELECT count(video_id) as number_of_pattern_occurances\n",
    "            FROM tblSegments \n",
    "            GROUP BY segment_structure\n",
    "            HAVING number_of_pattern_occurances > 1\n",
    "            ORDER BY number_of_pattern_occurances DESC;\n",
    "            \"\"\"\n",
    "    \n",
    "    sql_all = \"\"\"\n",
    "        SELECT count(video_id) as number_of_pattern_occurances\n",
    "        FROM tblSegments \n",
    "        GROUP BY segment_structure\n",
    "        ORDER BY number_of_pattern_occurances DESC;\n",
    "        \"\"\"\n",
    "\n",
    "    df_repeated = pd.read_sql(sql_repeated, db)\n",
    "    df_all = pd.read_sql(sql_all, db)\n",
    "\n",
    "finally:\n",
    "    db.close()\n",
    "\n",
    "print( \"\"\"\n",
    "        The number of repeated grammatical patterns is {}.\n",
    "        There are a total of {} unique grammatical patterns found in title segments.\"\"\"\n",
    "        .format(len(df_repeated.index), len(df_all.index)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "test = \"CREATE TABLE testMcTest (id INTEGER AUTOIMCREMENT PRIMARY KEY);\"\n",
    "\n",
    "run_sql(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
