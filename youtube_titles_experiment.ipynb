{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import csv\n",
    "import sys\n",
    "import nltk\n",
    "# import pprint\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "input_file = 'USvideos.csv'\n",
    "pos_file = 'parts_of_speech.csv'\n",
    "\n",
    "with open(input_file, 'r') as youtube_data:\n",
    "    masterdata_csv = list(csv.reader(youtube_data))\n",
    "\n",
    "with open(pos_file, 'r') as parts_of_speech:\n",
    "    pos_load = csv.reader(parts_of_speech)\n",
    "    pos_dict = {}\n",
    "    for row in pos_load:\n",
    "        pos_dict[row[0]] = row[1]\n",
    "\n",
    "def open_db(database):\n",
    "    db = sqlite3.connect(database)\n",
    "    db.execute('PRAGMA foreign_keys = ON;')\n",
    "    return db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Making an effort to decorate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sql_decorator(func):\n",
    "    \n",
    "    def sql_action(statement, opt_args = None):\n",
    "        db = None\n",
    "        try:\n",
    "            db = open_db('youtube.db')\n",
    "            c = db.cursor()\n",
    "            f = func(c, statement, opt_args)\n",
    "            if f:\n",
    "                return f\n",
    "                #hackey attempt to result of the function call just for fetchall\n",
    "            db.commit()\n",
    "\n",
    "        except OSError as err:\n",
    "            print(\"OS error: {}\".format(err))\n",
    "\n",
    "        finally:\n",
    "            db.rollback()\n",
    "            db.close()\n",
    "            \n",
    "\n",
    "    return sql_action\n",
    "\n",
    "@sql_decorator\n",
    "def run_sql(c, statement, opt_args):\n",
    "    if opt_args == None:\n",
    "        opt_args = []\n",
    "    c.execute(statement, opt_args)\n",
    "\n",
    "@sql_decorator\n",
    "def run_sql_many(c, statement, opt_args):\n",
    "    c.executemany(statement, opt_args)\n",
    "\n",
    "@sql_decorator\n",
    "def sql_fetchall(c, statement, opt_args):\n",
    "    if opt_args == None:\n",
    "        opt_args = []\n",
    "    c.execute(statement, opt_args)\n",
    "    return c.fetchall()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create database and master table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "try : \n",
    "    os.remove('youtube.db')\n",
    "except FileNotFoundError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_statement_master = '''\n",
    "    CREATE TABLE tblMasterData (\n",
    "    id INTEGER AUTOIMCREMENT PRIMARY KEY,\n",
    "    video_id TEXT,\n",
    "    trending_date TEXT,\n",
    "    title TEXT,\n",
    "    channel_title TEXT,\n",
    "    category_id INTEGER,\n",
    "    publish_time INTEGER,\n",
    "    tags TEXT,\n",
    "    views INTEGER,\n",
    "    likes INTEGER,\n",
    "    dislikes INTEGER,\n",
    "    comment_count INTEGER,\n",
    "    thumbnail_link TEXT,\n",
    "    comments_disabled TEXT,\n",
    "    ratings_disabled TEXT,\n",
    "    video_error_or_removed TEXT,\n",
    "    description TEXT)\n",
    "    '''\n",
    "\n",
    "run_sql(create_statement_master)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save column headers into a dictionary and delete column header row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = masterdata_csv[0]\n",
    "headers_dict = {}\n",
    "\n",
    "for count, value in enumerate(headers, 1):\n",
    "    headers_dict[value] = count -1\n",
    "# to keep from 0 indexing my columns\n",
    "\n",
    "del masterdata_csv[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load data into master table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_statement_master = '''\n",
    "    INSERT INTO tblMasterData\n",
    "    (video_id,\n",
    "    trending_date,\n",
    "    title,\n",
    "    channel_title,\n",
    "    category_id,\n",
    "    publish_time,\n",
    "    tags,\n",
    "    views,\n",
    "    likes,\n",
    "    dislikes,\n",
    "    comment_count,\n",
    "    thumbnail_link,\n",
    "    comments_disabled,\n",
    "    ratings_disabled,\n",
    "    video_error_or_removed,\n",
    "    description)\n",
    "    VALUES (?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?)\n",
    "    '''\n",
    "\n",
    "source = masterdata_csv\n",
    "\n",
    "run_sql_many(load_statement_master, source)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### After reviewing master data, create two tables based on function -- one static, one transactional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_statement_video = '''\n",
    "    CREATE TABLE tblVideos (\n",
    "    video_id TEXT PRIMARY KEY NOT NULL,\n",
    "    title TEXT NOT NULL,\n",
    "    channel_title TEXT NOT NULL,\n",
    "    publish_time INTEGER NOT NULL,\n",
    "    tags TEXT NOT NULL,\n",
    "    thumbnail_link TEXT NOT NULL,\n",
    "    comments_disabled TEXT NOT NULL,\n",
    "    ratings_disabled TEXT NOT NULL,\n",
    "    video_error_or_removed TEXT NOT NULL,\n",
    "    description TEXT NOT NULL)\n",
    "    '''\n",
    "\n",
    "run_sql(create_statement_video)\n",
    "\n",
    "create_statement_time = '''\n",
    "    CREATE TABLE tblTime (\n",
    "    id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "    video_id TEXT NOT NULL,\n",
    "    trending_date TEXT NOT NULL,\n",
    "    views INTEGER NOT NULL,\n",
    "    likes INTEGER NOT NULL,\n",
    "    dislikes INTEGER NOT NULL,\n",
    "    comment_count INTEGER NOT NULL,\n",
    "        FOREIGN KEY(video_id) REFERENCES tblVideos(video_id))\n",
    "    '''\n",
    "\n",
    "run_sql(create_statement_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Date is stored in hard-to-read format. Transform date column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for row in masterdata_csv:\n",
    "    date = row[headers_dict['trending_date']]\n",
    "    updated_date = \"20\" + date[:2]\n",
    "    updated_date += \"-\"\n",
    "    updated_date += date[6:]\n",
    "    updated_date += \"-\"\n",
    "    updated_date += date[3:5]\n",
    "    row[headers_dict['trending_date']] = updated_date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Titles contain characters + and &. These will not be parseable. Replacing with 'and'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "for row in masterdata_csv:\n",
    "    title = row[headers_dict['title']]\n",
    "    updated_title = re.sub('&|\\+', 'and', title)\n",
    "    row[headers_dict['title']] = updated_title"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Separate CSV file into lists to be loaded to each table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_data = []\n",
    "time_data = []\n",
    "\n",
    "for row in masterdata_csv:\n",
    "    video_entry = [row[headers_dict['video_id']], \n",
    "                   row[headers_dict['title']], \n",
    "                   row[headers_dict['channel_title']], \n",
    "                   row[headers_dict['publish_time']],\n",
    "                   row[headers_dict['tags']],\n",
    "                   row[headers_dict['thumbnail_link']],\n",
    "                   row[headers_dict['comments_disabled']],\n",
    "                   row[headers_dict['ratings_disabled']],\n",
    "                   row[headers_dict['video_error_or_removed']],\n",
    "                   row[headers_dict['description']]\n",
    "                  ]\n",
    "    \n",
    "    video_data.append(video_entry)\n",
    "    \n",
    "    time_entry = [row[headers_dict['video_id']], \n",
    "                  row[headers_dict['trending_date']], \n",
    "                  row[headers_dict['views']], \n",
    "                  row[headers_dict['likes']],\n",
    "                  row[headers_dict['dislikes']],\n",
    "                  row[headers_dict['comment_count']]\n",
    "                 ]\n",
    "    \n",
    "    time_data.append(time_entry)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load data to tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_statement_videos = '''\n",
    "    INSERT OR REPLACE INTO tblVideos\n",
    "    (video_id,\n",
    "    title,\n",
    "    channel_title,\n",
    "    publish_time,\n",
    "    tags,\n",
    "    thumbnail_link,\n",
    "    comments_disabled,\n",
    "    ratings_disabled,\n",
    "    video_error_or_removed,\n",
    "    description)\n",
    "    VALUES (?,?,?,?,?,?,?,?,?,?)\n",
    "    '''\n",
    "\n",
    "run_sql_many(load_statement_videos, video_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_statement_time = '''\n",
    "    INSERT INTO tblTime\n",
    "    (video_id,\n",
    "    trending_date,\n",
    "    views,\n",
    "    likes,\n",
    "    dislikes,\n",
    "    comment_count)\n",
    "    VALUES (?,?,?,?,?,?)\n",
    "    '''\n",
    "\n",
    "run_sql_many(load_statement_time, time_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test for foreign key failure, uncomment and run for evidence  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# failure_statement = \"INSERT INTO tblTime VALUES (?, ?, ?, ?, ?, ?, ?)\"\n",
    "# failure_params = (None, \"testy\", \"18.11.11\", 1, 1, 1, 1)\n",
    "\n",
    "# run_sql(failure_statement, failure_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Titles appear to come in multiple segments, divided by special characters. Create Segments table to store segments of each title for analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_statement_segments = '''\n",
    "    CREATE TABLE tblSegments (\n",
    "    segment_id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "    video_id TEXT NOT NULL,\n",
    "    segment_text TEXT NOT NULL,\n",
    "    segment_structure TEXT,\n",
    "        FOREIGN KEY(video_id) REFERENCES tblVideos(video_id))\n",
    "    '''\n",
    "\n",
    "run_sql(create_statement_segments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define classes to facilitate analaysis...and for practice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Video:\n",
    "    \n",
    "    def __init__(self, video_id, title):\n",
    "        self.video_id = video_id\n",
    "        self.title = title\n",
    "    \n",
    "    def longest_word(self):\n",
    "        longest_length = 0\n",
    "        longest_word = ''\n",
    "        for word in self.title:\n",
    "            if len(word) > longest_length:\n",
    "                longest_length = len(word)\n",
    "                longest_word = word\n",
    "            else:\n",
    "                continue\n",
    "        return longest_word\n",
    "\n",
    "    def title_segments(self):\n",
    "        list = re.compile(\"(?:\\||(?:\\s-\\s)|—|:|\\(|\\)|\\[|\\]|{|})+\").split(self.title)\n",
    "        # ?: indicates a non-capture group so delimiters aren't saved. Now I know.\n",
    "        list = filter(lambda x: x != None, list)\n",
    "        #  python is returning None where the delimiter was. Removing Nones.\n",
    "        segments = []\n",
    "        for text in list:\n",
    "            text = text.strip()\n",
    "            if text == \"\":\n",
    "                continue\n",
    "            segments.append(Segment(self.video_id, text))\n",
    "        return segments\n",
    "\n",
    "    \n",
    "class Segment:\n",
    "    \n",
    "    def __init__(self, video_id, text):\n",
    "        self.video_id = video_id\n",
    "        self._text = text\n",
    "        \n",
    "    def text(self):\n",
    "        lower_words = self._text.lower()\n",
    "        return re.sub('[^A-Za-z0-9\\s\\-\\']+', '', lower_words)\n",
    "    \n",
    "    def words(self):\n",
    "        list = self.text().split()\n",
    "        list = filter(lambda x: x != None, list)\n",
    "        #  python is returning None where the delimiter was. Removing Nones.\n",
    "        segments = []\n",
    "        for item in list:\n",
    "            text = item.strip()\n",
    "            if text == \"\":\n",
    "                continue\n",
    "            segments.append(text)\n",
    "        return segments\n",
    "    \n",
    "    def parts_of_speech(self):\n",
    "        text = nltk.word_tokenize(self.text())\n",
    "        tagged_text = nltk.pos_tag(text)\n",
    "        #  creates list of tuples with (word, part of speech)\n",
    "        return [item[1] for item in tagged_text]\n",
    "\n",
    "class Title_glob:\n",
    "    \n",
    "    def __init__(self, glob):\n",
    "        self.glob = glob\n",
    "    \n",
    "    def word_list(self):\n",
    "        return self.glob\n",
    "    \n",
    "    def frequency_distribution(self):\n",
    "        return nltk.FreqDist(self.glob)\n",
    "    \n",
    "    def lexical_diversity(self):\n",
    "        return len(set(self.glob)) / len(self.glob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Select data from Videos table to parse into segments, generate parts of speech for each, and commit to Segments table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('9wRQljFNDW8', \"Dion Lewis' 103-Yd Kick Return TD vs. Denver! | Can't-Miss Play | NFL Wk 10 Highlights\"), ('Om_zGhJLZ5U', 'TL;DW - Every DCEU Movie Before Justice League'), ('goP4Z5wyOlM', 'Iraq-Iran earthquake: Deadly tremor hits border region - BBC News'), ('8NHA23f7LvU', 'Jason Momoa Wows Hugh Grant With Some Dothraki | The Graham Norton Show'), ('IE-xepGLVt8', \"Mayo Clinic's first face transplant patient meets donor’s family\")]\n"
     ]
    }
   ],
   "source": [
    "titles_list = sql_fetchall(\"SELECT video_id, title FROM tblVideos;\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "segment_entries=[]\n",
    "\n",
    "for row in titles_list:\n",
    "    video = Video(row[0], row[1])\n",
    "    segments = video.title_segments()\n",
    "    for segment in segments:\n",
    "        segment_entry = []\n",
    "        segment_entry.append(segment.video_id)\n",
    "        segment_entry.append(segment.text())\n",
    "        segment_entry.append(\", \".join(segment.parts_of_speech()))\n",
    "        segment_entries.append(segment_entry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "insert_statement_segments = '''\n",
    "        INSERT INTO tblSegments\n",
    "        (video_id,\n",
    "        segment_text,\n",
    "        segment_structure)\n",
    "        VALUES (?,?, ?)\n",
    "        '''\n",
    "source = segment_entries\n",
    "\n",
    "run_sql_many(insert_statement_segments, source)\n",
    "\n",
    "# select_segments = run_query('SELECT * FROM tblSegments WHERE segment_id>=(abs(random()) % (SELECT max(segment_id)FROM tblSegments)) LIMIT 5')\n",
    "\n",
    "# select_segments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It doesn't know that all nouns aren't proper...bit of a bummer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "segments_list = sql_fetchall(\"SELECT video_id, segment_text, segment_id FROM tblSegments;\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Concatenate all segments for analysis as a whole"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lower_case(list):\n",
    "    return [w.lower() for w in list]\n",
    "\n",
    "all_segment_words = []\n",
    "\n",
    "for item in segments_list:\n",
    "    segment = Segment(item[0], item[1])\n",
    "    segment_words = segment.words()\n",
    "    for word in segment_words: \n",
    "        all_segment_words.append(word)\n",
    "\n",
    "all_segment_words = lower_case(all_segment_words)\n",
    "\n",
    "all_words = Title_glob(sorted(all_segment_words))\n",
    "# print(all_words.word_list())\n",
    "\n",
    "all_tokens = Title_glob(sorted(set(all_segment_words)))\n",
    "# print(all_tokens.word_list())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The X most common words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('the', 1627), ('and', 753), ('a', 748), ('to', 685), ('in', 538), ('of', 531), ('official', 486), ('with', 470), ('on', 405), ('2018', 361), ('video', 317), ('for', 285), ('i', 285), ('trailer', 276), ('how', 274), ('is', 267), ('my', 257), ('you', 240), ('from', 201), ('at', 194), ('vs', 178), ('2017', 175), ('new', 170), ('ft', 155), ('live', 144), ('what', 142), ('first', 141), ('2', 139), ('hd', 139), ('this', 139)]\n"
     ]
    }
   ],
   "source": [
    "freq_dist = all_words.frequency_distribution()\n",
    "\n",
    "print(freq_dist.most_common(30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "        The lexical diversity of the corpus is 0.21445316284025961. \n",
      "        This is generated by dividing the length of the set of unique words over the length of  the set of all words.\n"
     ]
    }
   ],
   "source": [
    "print(\"\"\"\n",
    "        The lexical diversity of the corpus is {}. \n",
    "        This is generated by dividing the length of the set of unique words over the length of  the set of all words.\"\"\"\n",
    "        .format(all_words.lexical_diversity()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The longest word in the corpus is brfxxccxxmnpcccclllmmnprxvclmnckssqlbb11116 with a length of 43 characters\n"
     ]
    }
   ],
   "source": [
    "longest_word = ''\n",
    "longest_length = 0\n",
    "\n",
    "for word in all_tokens.word_list():\n",
    "    if len(word) > longest_length:\n",
    "        longest_word = word\n",
    "        longest_length = len(word)\n",
    "\n",
    "print(\"The longest word in the corpus is {} with a length of {} characters\".format(longest_word, longest_length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(6, '07JQ4WZJIbg'),\n",
       " (6, 'IxF3mxWbdjw'),\n",
       " (6, 'JWH5KE1atAg'),\n",
       " (6, 'XiHiW4N7-bo'),\n",
       " (6, 'Yq4_YocuVeg'),\n",
       " (6, 'Zjp0mdMeIPU')]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "segments_count = sql_fetchall(\"\"\"\n",
    "            SELECT count(segment_id) as number_of_segments, video_id \n",
    "            FROM tblSegments \n",
    "            GROUP BY video_id \n",
    "            ORDER BY number_of_segments desc\n",
    "            LIMIT 6;\n",
    "            \"\"\")\n",
    "\n",
    "segments_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'mean'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-b4aca7c87bb1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m             \"\"\")\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'mean'"
     ]
    }
   ],
   "source": [
    "df = sql_fetchall(\"\"\"\n",
    "            SELECT count(segment_id) as number_of_segments, video_id \n",
    "            FROM tblSegments \n",
    "            GROUP BY video_id;\n",
    "            \"\"\")\n",
    "\n",
    "print(df.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The mean number of segments per title is 1.855141"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = sql_fetchall('''\n",
    "            SELECT count(video_id) as number_of_pattern_occurances, segment_structure\n",
    "            FROM tblSegments \n",
    "            GROUP BY segment_structure\n",
    "            ORDER BY number_of_pattern_occurances DESC\n",
    "            LIMIT 10;\n",
    "            ''')\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "repeated = \"\"\"\n",
    "        SELECT count(video_id) as number_of_pattern_occurances\n",
    "        FROM tblSegments \n",
    "        GROUP BY segment_structure\n",
    "        HAVING number_of_pattern_occurances > 1\n",
    "        ORDER BY number_of_pattern_occurances DESC;\n",
    "        \"\"\"\n",
    "\n",
    "every = \"\"\"\n",
    "    SELECT count(video_id) as number_of_pattern_occurances\n",
    "    FROM tblSegments \n",
    "    GROUP BY segment_structure\n",
    "    ORDER BY number_of_pattern_occurances DESC;\n",
    "    \"\"\"\n",
    "\n",
    "df_repeated = sql_fetchall(repeated)\n",
    "df_all = sql_fetchall(every)\n",
    "\n",
    "print( \"\"\"\n",
    "        The number of repeated grammatical patterns is {}.\n",
    "        There are a total of {} unique grammatical patterns found in title segments.\"\"\"\n",
    "        .format(len(df_repeated.index), len(df_all.index)))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
